#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯”è¾ƒå¹¶å‘ä¼˜åŒ–å‰åçš„æ€§èƒ½å·®å¼‚
"""

import time
import logging
from typing import Dict, List, Any
from src.summarizer import OntologySummarizer

# è®¾ç½®æ—¥å¿—çº§åˆ«ä»¥å‡å°‘è¾“å‡º
logging.basicConfig(level=logging.WARNING)

def create_test_data(num_clusters: int = 8, docs_per_cluster: int = 4) -> Dict[int, List[Dict[str, Any]]]:
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    test_clusters = {}
    
    topics = [
        "æœºå™¨å­¦ä¹ ç®—æ³•", "æ·±åº¦ç¥ç»ç½‘ç»œ", "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯", "è®¡ç®—æœºè§†è§‰ç³»ç»Ÿ",
        "æ•°æ®æŒ–æ˜æ–¹æ³•", "æ¨èç³»ç»Ÿç®—æ³•", "è¯­éŸ³è¯†åˆ«æŠ€æœ¯", "å›¾åƒå¤„ç†ç®—æ³•",
        "å¼ºåŒ–å­¦ä¹ æ¨¡å‹", "çŸ¥è¯†å›¾è°±æ„å»º", "ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿ", "æ–‡æœ¬åˆ†ç±»æŠ€æœ¯"
    ]
    
    for i in range(num_clusters):
        topic = topics[i % len(topics)]
        documents = []
        
        for j in range(docs_per_cluster):
            doc = {
                'id': f'doc_{i}_{j}',
                'title': f'{topic}ç ”ç©¶æ–‡æ¡£{j+1}',
                'context': f'è¿™æ˜¯å…³äº{topic}çš„è¯¦ç»†æŠ€æœ¯æ–‡æ¡£ã€‚{topic}æ˜¯äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ é¢†åŸŸçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œæ¶‰åŠå¤æ‚çš„ç®—æ³•è®¾è®¡ã€æ•°æ®ç»“æ„ä¼˜åŒ–ã€æ¨¡å‹è®­ç»ƒç­–ç•¥ã€æ€§èƒ½è¯„ä¼°æ–¹æ³•ç­‰å¤šä¸ªæŠ€æœ¯å±‚é¢ã€‚è¯¥æŠ€æœ¯åœ¨å®é™…å·¥ç¨‹åº”ç”¨ä¸­å…·æœ‰é‡è¦ä»·å€¼ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºè‡ªåŠ¨åŒ–ç³»ç»Ÿã€æ™ºèƒ½æ¨èã€å†³ç­–æ”¯æŒã€æ¨¡å¼è¯†åˆ«ç­‰åº”ç”¨åœºæ™¯ã€‚æŠ€æœ¯å®ç°éœ€è¦è€ƒè™‘è®¡ç®—å¤æ‚åº¦ã€å†…å­˜ä½¿ç”¨ã€å¹¶å‘å¤„ç†ã€é”™è¯¯å¤„ç†ç­‰å·¥ç¨‹å› ç´ ã€‚'
            }
            documents.append(doc)
        
        test_clusters[i] = documents
    
    return test_clusters

def test_concurrent_performance():
    """æµ‹è¯•å¹¶å‘ç‰ˆæœ¬æ€§èƒ½"""
    print("ğŸš€ æµ‹è¯•å¹¶å‘ä¼˜åŒ–ç‰ˆæœ¬...")
    
    test_data = create_test_data(num_clusters=8, docs_per_cluster=3)
    
    # ä½¿ç”¨å¹¶å‘ä¼˜åŒ–çš„é…ç½®
    summarizer = OntologySummarizer(
        output_dir="performance_results",
        max_concurrent_llm=5,     # å¹¶å‘LLMè°ƒç”¨
        max_concurrent_embedding=3  # å¹¶å‘åµŒå…¥è®¡ç®—
    )
    
    start_time = time.time()
    result = summarizer.summarize_clusters(test_data)
    end_time = time.time()
    
    processing_time = end_time - start_time
    
    return {
        'version': 'concurrent',
        'clusters': len(test_data),
        'total_docs': sum(len(docs) for docs in test_data.values()),
        'output_ontologies': len(result),
        'processing_time': processing_time,
        'avg_time_per_cluster': processing_time / len(test_data)
    }

def test_sequential_simulation():
    """æ¨¡æ‹Ÿé¡ºåºå¤„ç†ç‰ˆæœ¬ï¼ˆé€šè¿‡é™åˆ¶å¹¶å‘æ•°ä¸º1ï¼‰"""
    print("ğŸŒ æµ‹è¯•é¡ºåºå¤„ç†ç‰ˆæœ¬...")
    
    test_data = create_test_data(num_clusters=8, docs_per_cluster=3)
    
    # é™åˆ¶å¹¶å‘æ•°ä¸º1æ¥æ¨¡æ‹Ÿé¡ºåºå¤„ç†
    summarizer = OntologySummarizer(
        output_dir="performance_results",
        max_concurrent_llm=1,      # é¡ºåºLLMè°ƒç”¨
        max_concurrent_embedding=1   # é¡ºåºåµŒå…¥è®¡ç®—
    )
    
    start_time = time.time()
    result = summarizer.summarize_clusters(test_data)
    end_time = time.time()
    
    processing_time = end_time - start_time
    
    return {
        'version': 'sequential',
        'clusters': len(test_data),
        'total_docs': sum(len(docs) for docs in test_data.values()),
        'output_ontologies': len(result),
        'processing_time': processing_time,
        'avg_time_per_cluster': processing_time / len(test_data)
    }

def compare_performance():
    """æ¯”è¾ƒæ€§èƒ½"""
    print("=== æœ¬ä½“æ€»ç»“å™¨å¹¶å‘æ€§èƒ½æ¯”è¾ƒæµ‹è¯• ===\n")
    
    # æµ‹è¯•é¡ºåºå¤„ç†ç‰ˆæœ¬
    sequential_result = test_sequential_simulation()
    
    print(f"é¡ºåºå¤„ç†å®Œæˆ: {sequential_result['processing_time']:.2f} ç§’\n")
    
    # æµ‹è¯•å¹¶å‘å¤„ç†ç‰ˆæœ¬
    concurrent_result = test_concurrent_performance()
    
    print(f"å¹¶å‘å¤„ç†å®Œæˆ: {concurrent_result['processing_time']:.2f} ç§’\n")
    
    # è®¡ç®—æ€§èƒ½æå‡
    speedup = sequential_result['processing_time'] / concurrent_result['processing_time']
    time_saved = sequential_result['processing_time'] - concurrent_result['processing_time']
    efficiency_gain = ((sequential_result['processing_time'] - concurrent_result['processing_time']) / sequential_result['processing_time']) * 100
    
    # è¾“å‡ºè¯¦ç»†æ¯”è¾ƒç»“æœ
    print("=" * 60)
    print("ğŸ“Š æ€§èƒ½æ¯”è¾ƒç»“æœ")
    print("=" * 60)
    
    print(f"æµ‹è¯•æ•°æ®è§„æ¨¡:")
    print(f"  â€¢ èšç±»æ•°é‡: {sequential_result['clusters']}")
    print(f"  â€¢ æ–‡æ¡£æ€»æ•°: {sequential_result['total_docs']}")
    print(f"  â€¢ è¾“å‡ºæœ¬ä½“æ•°: {sequential_result['output_ontologies']}")
    print()
    
    print(f"å¤„ç†æ—¶é—´å¯¹æ¯”:")
    print(f"  â€¢ é¡ºåºå¤„ç†: {sequential_result['processing_time']:.2f} ç§’")
    print(f"  â€¢ å¹¶å‘å¤„ç†: {concurrent_result['processing_time']:.2f} ç§’")
    print(f"  â€¢ èŠ‚çœæ—¶é—´: {time_saved:.2f} ç§’")
    print()
    
    print(f"æ€§èƒ½æŒ‡æ ‡:")
    print(f"  â€¢ é€Ÿåº¦æå‡: {speedup:.2f}x")
    print(f"  â€¢ æ•ˆç‡æå‡: {efficiency_gain:.1f}%")
    print(f"  â€¢ å¹³å‡æ¯èšç±»å¤„ç†æ—¶é—´:")
    print(f"    - é¡ºåº: {sequential_result['avg_time_per_cluster']:.2f} ç§’")
    print(f"    - å¹¶å‘: {concurrent_result['avg_time_per_cluster']:.2f} ç§’")
    print()
    
    # æ€§èƒ½è¯„ä¼°
    if speedup >= 2.0:
        grade = "ğŸ† ä¼˜ç§€"
    elif speedup >= 1.5:
        grade = "ğŸ¥ˆ è‰¯å¥½"
    elif speedup >= 1.2:
        grade = "ğŸ¥‰ ä¸€èˆ¬"
    else:
        grade = "âš ï¸  éœ€è¦ä¼˜åŒ–"
    
    print(f"å¹¶å‘ä¼˜åŒ–è¯„çº§: {grade}")
    print(f"å»ºè®®: ", end="")
    
    if speedup >= 2.0:
        print("å¹¶å‘ä¼˜åŒ–æ•ˆæœæ˜¾è‘—ï¼Œå»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨ã€‚")
    elif speedup >= 1.5:
        print("å¹¶å‘ä¼˜åŒ–æ•ˆæœè‰¯å¥½ï¼Œå¯ä»¥æŠ•å…¥ä½¿ç”¨ã€‚")
    elif speedup >= 1.2:
        print("æœ‰ä¸€å®šæ”¹å–„ï¼Œå¯è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–å¹¶å‘ç­–ç•¥ã€‚")
    else:
        print("å¹¶å‘æ•ˆæœä¸æ˜æ˜¾ï¼Œå»ºè®®æ£€æŸ¥ç“¶é¢ˆæˆ–è°ƒæ•´å¹¶å‘å‚æ•°ã€‚")
    
    print("=" * 60)
    
    return {
        'sequential': sequential_result,
        'concurrent': concurrent_result,
        'speedup': speedup,
        'efficiency_gain': efficiency_gain
    }

if __name__ == "__main__":
    try:
        results = compare_performance()
        print(f"\nâœ… æ€§èƒ½æ¯”è¾ƒæµ‹è¯•å®Œæˆï¼")
        print(f"å¹¶å‘ä¼˜åŒ–å®ç°äº† {results['speedup']:.2f}x çš„æ€§èƒ½æå‡")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()